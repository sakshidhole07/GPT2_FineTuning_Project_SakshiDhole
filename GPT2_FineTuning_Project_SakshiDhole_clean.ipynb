{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning GPT-2 for Context-Aware Text Generation\n",
        "**Internship Project â€“ Sakshi Dhole**\n",
        "\n",
        "**Objective:**  \n",
        "To fine-tune a pre-trained GPT-2 model on a custom dataset to generate coherent and contextually relevant text.\n"
      ],
      "metadata": {
        "id": "f0G0csUyCePl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required Libraries\n",
        "We will install the Hugging Face Transformers, Datasets, and PyTorch libraries.\n"
      ],
      "metadata": {
        "id": "wL-SkM4YCmNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY-xA6esBrmn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Libraries\n",
        "Import all necessary libraries for model loading, training, and dataset handling.\n"
      ],
      "metadata": {
        "id": "M7vmeCAhDT5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n",
        "from datasets import Dataset\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "ntUi2FGaDYVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load GPT-2 Model & Tokenizer\n",
        "We will use the `distilgpt2` pre-trained model.\n",
        "We also set the padding token and resize embeddings for GPT-2.\n"
      ],
      "metadata": {
        "id": "l0KA8k7JDrLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Set pad token and resize embeddings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "07Q7mjFBDtqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create Custom Dataset\n",
        "We create a simple dataset of 5 sentences for demonstration.\n"
      ],
      "metadata": {
        "id": "F-e8FNXGD-7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"text\": [\n",
        "        \"Hello, how are you?\",\n",
        "        \"Artificial Intelligence is transforming the world.\",\n",
        "        \"Machine learning allows computers to learn from data.\",\n",
        "        \"Deep learning is a subset of machine learning.\",\n",
        "        \"Natural language processing helps machines understand text.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "1ntxBOySEAIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Tokenize Dataset\n",
        "We convert the text data into tokens that GPT-2 can understand.\n",
        "We also pad and truncate sequences to a maximum length of 128.\n"
      ],
      "metadata": {
        "id": "j_WRjJfJEJKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset\n"
      ],
      "metadata": {
        "id": "kb7jqs-_EKZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Prepare Data Collator\n",
        "We use `DataCollatorForLanguageModeling` to create batches for GPT-2.\n",
        "MLM is False because GPT-2 uses causal language modeling.\n"
      ],
      "metadata": {
        "id": "Um2d5FRjERzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "YvksZ48nES8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Set Training Parameters\n",
        "Define the training parameters such as number of epochs, batch size, output directory, and logging.\n"
      ],
      "metadata": {
        "id": "zkyIUqhFEepa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "om-bZl46EgBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Train GPT-2 Model\n",
        "We use the Hugging Face `Trainer` API to fine-tune the model on our dataset.\n"
      ],
      "metadata": {
        "id": "-RwehP9yEn4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "RkwB81xXEpaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Save Fine-Tuned Model\n",
        "After training, we save the model and tokenizer locally as a zip file for submission.\n"
      ],
      "metadata": {
        "id": "6IRyP1DSE36E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n",
        "\n",
        "# Create a zip file\n",
        "!zip -r fine_tuned_gpt2.zip fine_tuned_gpt2\n"
      ],
      "metadata": {
        "id": "JGtSpAmgE5Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Generate Text Using Fine-Tuned Model\n",
        "We can now test the model to see if it generates coherent text similar to our dataset.\n"
      ],
      "metadata": {
        "id": "OKt_kI3DFKfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "prompt = \"Artificial Intelligence is\"\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "rr6xeq3WFOTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "- Fine-tuned GPT-2 on a small custom dataset\n",
        "- Successfully generated coherent text\n",
        "- Model and tokenizer saved as `fine_tuned_gpt2.zip`\n",
        "- Ready for submission as an internship project\n"
      ],
      "metadata": {
        "id": "jRVI3UyiFdzW"
      }
    }
  ]
}